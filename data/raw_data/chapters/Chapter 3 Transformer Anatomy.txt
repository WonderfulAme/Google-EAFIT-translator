Chapter 3.
Transformer Anatomy 
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form—the author’s raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.
This will be the 3rd chapter of the final book.
Please note that the GitHub repo will be made active later on.
If you have comments about how we might improve the content and/or examples in this book, or if you notice missing material within this chapter, please reach out to the editor at mpotter@oreilly.com.
Now that we’ve seen what it takes to fine-tune and evaluate a transformer in Chapter 2, let’s take a look at how they work under the hood.
In this chapter we’ll explore what the main building blocks of transformer models look like and how to implement them using PyTorch.
We first focus on building the attention mechanism and then add the bits and pieces necessary to make a transformer encoder work.
We also have a brief look at the architectural differences between the encoder and decoder modules.
By the end of this chapter you will be able to implement a simple transformer model yourself! 
While a deep, technical understanding of the transformer architecture is generally not necessary to use the Transformers library and fine-tune models to your use-case, it can help understand and navigate the limitations of the architecture or expand it to new domains.
This chapter also introduces a taxonomy of transformers to help us understand the veritable zoo of models that has emerged in recent years.
Before diving into the code, let’s start with an overview of the original architecture that kick-started the transformer revolution.
The Transformer 
As we saw in Chapter 1, the original Transformer is based on the encoderdecoder architecture that is widely used for tasks like machine translation, where a sequence of words is translated from one language to another.
This architecture consists of two components: The encoder converts an input sequence of tokens into a sequence of embedding vectors, often called the hidden state or context, the decoder uses the encoder’s hidden state to iteratively generate an output sequence of tokens, one token at a time.
Before the arrival of transformers, the building blocks of the encoder and decoder were typically recurrent neural networks such as LSTMs,1 augmented with a mechanism called attention.
Instead of using a fixed hidden state for the whole input sequence, attention allowed the decoder to assign a different amount of weight or “attention” to each of the encoder states at every decoding timestep.
By focusing on which input tokens are most relevant at each timestep, these models were able to learn non-trivial alignments between the words in a generated translation and those in a source sentence.
For example, Figure 3-1 visualizes the attention weights for an English to French translation model and shows hows the decoder is able to correctly align the words “zone” and “Area” which are ordered differently in the two languages.
Figure 3-1.
RNN encoder-decoder alignment of words in the source language (English) and generated translation (French), where each pixel denotes an attention weight.
Although attention produced much better translations, there was still a major shortcoming with using recurrent models for the encoder and decoder: the computations are inherently sequential which prevents parallelization across tokens in the input sequence.
With the Transformer, a new modeling paradigm was introduced: dispense with recurrence altogether, and instead rely entirely on a special form of attention called self-attention.
We’ll cover self-attention in more detail later, but in simple terms it is like attention except that it operates on hidden states of the same type.
So, although the building blocks changed in the Transformer, the general architecture remained that of an encoder-decoder as shown in Figure 3-2.
This architecture can be trained to convergence faster than recurrent models and paved the way for many of the recent breakthroughs in NLP.
Figure 3-2.
Encoder-decoder architecture of the Transformer, with the encoder shown in the upper half of the figure and the decoder in the lower half.
We’ll look at each of the building blocks in detail shortly, but we can already see a few things in Figure 3-2 that characterize the transformer architecture: The input text is tokenized and converted to token embeddings using the techniques we encountered in Chapter 2.
Since the attention mechanism is not aware of the relative positions of the tokens, we need a way to inject some information about token positions in the input to model the sequential nature of text.
The token embeddings are thus combined with positional embeddings that contain positional information for each token.
The encoder consists of a stack of encoder layers or “blocks” which is analogous to stacking convolutional layers in computer vision.
The same is true for the decoder which has its own stack of decoder layers.
The encoder’s output is fed to each decoder layer, which then generates a prediction for the most probable next token in the sequence.
The output of this step is then fed back into the decoder to generate the next token, and so on until a special end-ofsequence token is reached.
The Transformer architecture was originally designed for sequence-tosequence tasks like machine translation, but both the encoder and decoder submodules were soon adapted as stand-alone models.
Although there are hundreds of different transformer models, most of them belong to one of three types: Encoder-only These models convert an input sequence of text into a rich numerical representation that is well suited for tasks like text classification or named entity recognition.
BERT and its variants like RoBERTa and DistilBERT belong to this class of architectures.
Decoder-only  Given a prompt of text like “Thanks for lunch, I had a …”, these models will auto-complete the sequence by iteratively predicting the most probable next word.
The family of GPT models belong to this class.
Encoder-decoder Used for modeling complex mappings from one sequence of text to another.
Suitable for machine translation and summarization.
The Transformer, BART and T5 models belong to this class.
NOTE In reality, the distinction between applications for decoder-only versus encoder-only architectures is a bit blurry.
For example, decoder-only models like those in the GPT family can be primed for tasks like translation that are conventionally thought of as a sequence-to-sequence task.
Similarly, encoder-only models like BERT can be applied to summarization tasks that are usually associated with encoder-decoder or decoder-only models.
Now that we have a high-level understanding of the Transformer architecture, let’s take a closer look at the inner workings of the encoder.
Transformer Encoder 
As we saw earlier, the Transformer’s encoder consists of many encoder layers stacked next to each other.
As illustrated in Figure 3-3, each encoder layer receives a sequence of embeddings and feeds them through the following sub-layers: A multi-head self-attention layer.
A feed-forward layer.
The output embeddings of each encoder layer have the same size as the inputs and we’ll soon see that the main role of the encoder stack is to “update” the input embeddings to produce representations that encode some contextual information in the sequence.
Figure 3-3.
Zooming into the encoder layer.
Each of these sub-layers also has a skip connection and layer normalization, which are standard tricks to train deep neural networks effectively.
But to truly understand what makes a transformer work we have to go deeper.
Let’s start with the most important building block: the self-attention layer.
Self-Attention 
As we discussed earlier in this chapter, self-attention is a mechanism that allows neural networks to assign a different amount of weight or “attention” to each element in a sequence.
For text sequences, the elements are token embeddings like the ones we encountered in Chapter 2, where each token is mapped to a vector of some fixed dimension.
For example, in BERT each token is represented as a 768-dimensional vector.
The “self” part of selfattention refers to the fact that these weights are computed for all hidden states in the same set, e.g. all the hidden states of the encoder.
By contrast, the attention mechanism associated with recurrent models involves computing the relevance of each encoder hidden state to the decoder hidden state at a given decoding timestep.
The main idea behind self-attention is that instead of using a fixed embedding for each token, we can use the whole sequence to compute a weighted average of each embedding.
A simplified way to formulate this is to say that given a sequence of token embeddings x self-attention produces a sequence of new embeddings y, where each y is a linear combination of all the x :  The coefficients w are called attention weights and are normalized so that to see why averaging the token embeddings might be a good idea, consider what comes to mind when you see the word “flies”.
You might think of an annoying insect, but if you were given more context like “time flies like an arrow” then you would realize that “flies” refers to the verb instead.
Similarly, we can create a representation for “flies” that incorporates this context by combining all the token embeddings in different proportions, perhaps by assigning a larger weight w to the token embeddings for “time” and “arrow”.
Embeddings that are generated in this way are called contextualized embeddings and predate the invention of transformers with language models like ELMo4.
A cartoon of the process is shown in Figure 3-4 where we illustrate how, depending on the context, two different representations for “flies” can be generated via self-attention.
Figure 3-4.
Cartoon of how self-attention updates raw token embeddings (upper) into contextualized embeddings (lower) to create representations that incorporate information from the whole sequence.
Let’s now take a look at how we can calculate the attention weights.
Scaled Dot-Product 
Attention 
There are several ways to implement a self-attention layer, but the most common one is scaled dot-product attention from thettention is All You Need paper where the Transformer was introduced.
There are four main steps needed to implement this mechanism: Create query, key, and value vectors Each token embedding is projected into three vectors called query, key, and value.
Compute attention scores 
Determine how much the query and key vectors relate to each other using a similarity function.
As the name suggests, the similarity function for scaled dot-product attention is a dot-product matrix multiplication of the embeddings.
Queries and keys that are similar will have a large dotproduct, while those that don’t share much in common will have little to no overlap.
The outputs from this step are called the attention scores and for a sequence with n input tokens, there is a corresponding n × n matrix of attention scores.
Compute attention weights Dot-products can in general produce arbitrarily large numbers which can destabilize the training process.
To handle this, the attention scores are first multiplied by a scaling factor and then normalized with a softmax to ensure all the column values sum to one.
The resulting n × n matrix now contains all the attention weights w.
Update the token embeddings 
Once the attention weights are computed, we multiply them by the value vector to obtain an updated representation for embedding.
We can visualize how the attention weights are calculated with a nifty library called BertViz.
This library provides several functions that can be used for visualizing different aspects of attention in Transformers models.
To visualize the attention weights, we can use the neuron_view module which traces the computation of the weights to show how the query and key vectors are combined to produce the final weight.
Since BertViz needs to tap into the attention layers of the model, we’ll instantiate our BERT checkpoint with their model class and then use the show function to generate the interactive visualization:  DEMYSTIFYING QUERIES, KEYS, AND VALUES 
The notion of query, key, and value vectors can be a bit cryptic the first time you encounter them - for instance, why are they called that?
The origin of these names is inspired from information retrieval systems, but we can motivate their meaning with a simple analogy: imagine that you’re at the supermarket buying all the ingredients necessary for your dinner.
From the dish’s recipe, each of the required ingredients can be thought of as a query, and as you scan through the shelves you look at the labels (keys) and check if it matches an ingredient on your list (similarity function).
If you have a match then you take the item (value) from the shelf.
In this example, we only get one grocery item for every label that matches the ingredient.
Self-attention is a more abstract and “smooth” version of this: every label in the supermarket matches the ingredient to the extent to which each key matches the query.
Let’s take a look at this process in more detail by implementing the diagram of operations to compute scaled dot-product attention as shown in Figure 35.
Figure 3-5.
Operations in scaled dot-product attention.
The first thing we need to do is tokenize the text, so let’s use our tokenizer to extract the input IDs:  As we saw in Chapter 2, each token in the sentence has been mapped to a unique ID in the tokenizer’s vocabulary.
To keep things simple, we’ve also excluded the [CLS] and [SEP] tokens.
Next we need to create some dense embeddings.
In PyTorch, we can do this by using a torch.
Embedding layer that acts as a lookup table for each input ID:  Here we’ve used the AutoConfig class to load the config.json file associated with the bert-base-uncased checkpoint.
In Transformers, every checkpoint is assigned a configuration file that specifies various  hyperparameters like vocab_size and hidden_size, which in our example shows us that each input ID will be mapped to one of the 30,522 embedding vectors stored in nn.Embedding, each with a size of 768.
Now we have our lookup table we can generate the embeddings by feeding the input IDs:  This has given us a tensor of size (batch_size, seq_len, hidden_dim), just like we saw in Chapter 2.
We’ll postpose the positional encodings for later, so the next step is to create the query, key, and value vectors and calculate the attention scores using the dot-product as the similarity function:  We’ll see later that the query, key, and value vectors are generated by applying independent weight matrices W to the embeddings, but for now we’ve kept them equal for simplicity.
In scaled dot-product attention, the dot-products are scaled by the size of the embedding vectors so that we don’t get too many large numbers during training that can cause problems with back propagation.
NOTE The torch.bmm function performs a batch matrix-matrix product that simplifies the computation of the attention scores where the query and key vectors have size (batch_size, seq_len, hidden_dim).
If we ignored the batch dimension we could calculate the dot product between each query and key vector by simply transposing the key tensor to have shape (hidden_dim, seq_len) and then using the matrix product to collect all the dot-products in a (seq_len, seq_len) matrix.
Since we want to do this for all sequences in the batch independently, we use torch.bmm which simply prepends the batch dimension to the matrix dimensions.
This has created a 5 × 5 matrix of attention scores.
Next we normalize them by applying a softmax so the sum over each column is equal to one.
The final step is to multiply the attention weights by the values.
And that’s it - we’ve gone through all the steps to implement a simplified form of self-attention!
Notice that the whole process is just two matrix multiplications and a softmax, so next time you think of “self-attention” you can mentally remember that all we’re doing is just a fancy form of averaging.
Let’s wrap these steps in a function that we can use later.
Our attention mechanism with equal query and key vectors will assign a very large score to identical words in the context, and in particular to the current word itself: the dot product of a query with itself is always 1.
But in practice the meaning of a word will be better informed by complementary words in the context than by identical words, e.g. the meaning of “flies” is better defined by incorporating information from “time” and “arrow” than by another mention of “flies”.
How can we promote this behavior?
Let’s allow the model to create a different set of vectors for the query, key and value of a token by using three different linear projections to project our initial token vector into three different spaces.
Multi-Headed Attention 
In our simple example, we only used the embeddings “as is” to compute the attention scores and weights, but that’s far from the whole story.
In practice, the self-attention layer applies three independent linear transformations to each embedding to generate the query, key, and value vectors.
These transformations project the embeddings and each projection carries its own set of learnable parameters, which allows the self-attention layer to focus on different semantic aspects of the sequence.
It also turns out to be beneficial to have multiple sets of linear projections, each one representing a so-called attention head.
The resulting multiheaded attention layer is illustrated in Figure 3-6.
Figure 3-6.
Multi-headed attention.
Let’s implement this layer by first coding up a single attention head.
Here we’ve initialized three independent linear layers that apply matrix multiplication to the embedding vectors to produce tensors of size (batch_size, seq_len, head_dim) where head_dim is the dimension we are projecting into.
Although head_dim does not have to be smaller than the embedding dimension embed_dim of the tokens, in practice it is chosen to be a multiple of embed_dim so that the computation across each head is constant.
For example in BERT has 12 attention heads, so the dimension of each head is 768/12 = 64.
Now that we have a single attention head, we can concatenate the outputs of each one to implement the full multi-headed attention layer.
Notice that the concatenated output from the attention heads is also fed through a final linear layer to produce an output tensor of size (batch_size, seq_len, hidden_dim) that is suitable for the feed forward network downstream.
As a sanity check, let’s see if the multiheaded attention produces the expected shape of our inputs.
It works!
To wrap up this section on attention, let’s use BertViz again to visualise the attention for two different uses of the word “flies”.
Here we can use the head_view function from BertViz by computing the attentions, tokens and indicating where the sentence boundary lies.
This visualization shows the attention weights as lines connecting the token whose embedding is getting updated (left), with every word that is being attended to (right).
The intensity of the lines indicates the strength of the attention weights, with values close to 1 dark, and faint lines close to zero.
In this example, the input consists of two sentences and the [CLS] and [SEP] tokens are the special tokens in BERT’s tokenizer that we encountered in Chapter 2.
One thing we can see from the visualization is the attention weights are strongest between words that belong to the same sentence, which suggests BERT can tell that it should attend to words in the same sentence.
However, for the word “flies” we can see that BERT has identified “arrow” and important in the first sentence and “fruit” and “banana” in the second.
These attention weights allow the model to distinguish the use of “flies” as a verb or noun, depending on the context it occurs!
Now that we’ve covered attention, let’s take a look at implementing the missing piece of the encoder layer: position-wise feed forward networks.
Feed Forward Layer 
The feed forward sub-layer in the encoder and decoder is just a simple 2layer fully-connected neural network, but with a twist; instead of processing the whole sequence of embeddings as a single vector, it processes each embedding independently.
For this reason, this layer is often referred to as a position-wise feed forward layer.
These position-wise feed forward layers are sometimes also referred to as a one-dimensional convolution with kernel size of one, typically by people with a computer vision background (e.g. the OpenAI GPT codebase uses this nomenclature).
A rule of thumb from the literature is to pick the hidden size of the first layer to be four times the size of the embeddings and a GELU activation function is most commonly used.
This is where most of the capacity and memorization is hypothesized to happen and the part that is most often scaled when scaling up the models.
We now have all the ingredients to create a fully-fledged transformer encoder layer!
The only decision left to make is where to place the skip connections and layer normalization.
Let’s take a look and how this affect the model architecture.
Putting It All Together 
When it comes to placing the layer normalization in the encoder or decoder layers of a transformer, there are two main choices adopted in the literature: Post layer normalization This is the arrangement from the Transformer paper and places layer normalization in between the skip connections.
This arrangement is tricky to train from scratch as the gradients can diverge.
For this reason, you will often see a concept known as learning rate warm-up, where the learning rate is gradually increased from a small value to some maximum during training.
Pre layer normalization  
The most common arrangement found in the literature and places layer normalization in between the skip connections.
Tends to be much more stable during training and does not usually require learning rate warmup.
The difference between the two arrangements is illustrated in Figure 3-7.
Figure 3-7.
Different arrangements of layer normalization in a transformer encoder layer.
Let’s now test this with our input embeddings.
It works! 
We’ve now implemented our very first transformer encoder layer from scratch! 
In principle we could now pass the input embeddings through the encoder layer.
However, there is a caveat with the way we setup the encoder layers: they are totally invariant to the position of the tokens.
Since the multi-head attention layer is effectively a fancy weighted sum, there is no way to encode the positional information in the sequence.
Luckily there is an easy trick to incorporate positional information with positional encodings.
Let’s take a look.
Positional Embeddings  Positional embeddings are based on a simple, yet very effective idea: augment the token embeddings with a position-dependent pattern of values arranged in a vector.
If the pattern is characteristic for each position, the attention heads and feed-forward layers in each stack can learn to incorporate positional information in their transformations.
There are several ways to achieve this and one of the most popular approaches, especially when the pretraining dataset is sufficiently large, is to use a learnable pattern.
This works exactly the same way as the token embeddings but using the position index instead of the token ID as input.
With that approach an efficient way of encoding the position of tokens is learned during pretraining.
Let’s create a custom Embeddings module that combines a token embedding layer that projects the input_ids to a dense hidden state together with the positional embedding that does the same for position_ids.
The resulting embedding is simply the sum of both embeddings.
We see that the embedding layer now creates a single, dense embedding for each token.
While learnable position embeddings are easy to implement and widely used there are several alternatives: Absolute positional representations The Transformer model uses static patterns to encode the position of the tokens.
The pattern consists of modulated sine and cosine signals and works especially well in the low data regime.
Relative positional representations 
Although absolute positions are important one can argue that for computing a token embedding mostly the relative position to the token is important.
Relative positional representations follow that intuition and encode the relative positions between tokens.
Models such as DeBERTa use such representations.
Rotary position embeddings By combining the idea of absolute and relative positional representations rotary position embeddings achieve excellent results on many tasks.
A recent example of rotary position embeddings in action is GPT-Neo.
Let’s put it all together now by building the full transformer encoder by combining the embeddings with the encoder layers.
We can see that we get a hidden state for each token in the batch.
This output format makes the architecture very flexible and we can easily adapt it for various applications such as predicting missing tokens in masked langauge modeling or predicting start and end position of an answer in question-answering.
Let’s see how we can build a classifier with the encoder like the one we used in Chapter 2 in the following section.
Bodies and Heads 
So now that we have a full transformer encoder model we would like to build a classifier with it.
The model is usually divided into a task independant body and a task specific head.
What we’ve built so far is the body and we now need to attach a classification head to that body.
Since we have a hidden state for each token but only need to make one prediction there are several option how to approach this.
Traditionally, the first token in such models is used for the prediction and we can attach a dropout and linear layer to make a classification prediction.
The following class extends the existing encoder for sequence classification.
Before initializing the model we need to define how many classes we would like to predict.
That is exactly what we have been looking for.
For each example in the batch we get the un-normalized logits for each class in the output.
This corresponds to the BERT model that we used in Chapter 2 to detect emotions in tweets.
This concludes our analysis of the encoder, so let’s now cast our attention (pun intended!) to the decoder.
Transformer Decoder 
As illustrated in Figure 3-8, the main difference between the decoder and encoder is that the decoder has two attention sublayers: Masked multi-head attention ensures that the tokens we generate at each timestep are only based on the past outputs and the current token being predicted.
Without this, the decoder could cheat during training by simply copying the target translations, so masking the inputs ensures the task is not trivial.
Encoder-decoder attention Performs multi-head attention over the output key and value vectors of the encoder stack, with the intermediate representation of the decoder acting as the queries.
This way the encoder-decoder attention layer learns how to relate tokens from two different sequences such as two different languages.
Figure 3-8.
Zooming into the Transformer decoder layer.
Let’s take a look at the modifications we need to include masking in selfattention, and leave the implementation of the encoder-decoder attention layer as a homework problem.
The trick with masked self-attention is to introduce a mask matrix with ones on the lower diagonal and zeros above.
Here we’ve used PyTorch’s tril function to create the lower triangular matrix.
Once we have this mask matrix, we can the prevent each attention head from peeking at future tokens by using torch.Tensor.masked_fill to replace all the zeros with negative infinity.
By setting the upper values to negative infinity, we guarantee that the attention weights are all zero once we take the softmax over the scores because e = 0.
We can easily include this masking behavior with a small change to our scaled dot-product attention function that we implemented earlier in this chapter.
From here it is a simple matter to build up the decoder layer and we point the reader to the excellent implementation of minGPT by Andrej Karpathy for details.
Okay this was quite a lot of technical detail, but now we have a good understanding on how every piece of the Transformer architecture works.
Let’s round out the chapter by stepping back a bit and looking at the landscape of different transformer models and how they relate to each other.
Meet the Transformers
As we have seen in this chapter there are three main architectures for transformer models: encoders, decoders, and encoder-decoders.
The initial success of the early transformer models triggered a Cambrian explosion in model development as researchers built models on various datasets of different size and nature, used new pretraining objectives, and tweaked the architecture to further improve performance.
Although the zoo of models is still growing fast, the wide variety of models can still be divided into the three categories of encoders, decoders, and encoder-decoders.
In this section we’ll provide a brief overview of the most important transformer models.
Let’s start by taking a look at the transformer family tree.
The Transformer Tree of Life 
Over time, each of three main architecture have undergone an evolution of their own which is illustrated in Figure 3-9 where a few of the most prominent models and their descendants is shown.
Figure 3-9.
An overview of some of the most prominent transformer architectures.
With over 50 different architectures included in Transformers, this family tree by no means provides a complete overview of all the existing architectures, and simply highlights a few of the architectural milestones.
We’ve covered the Transformer in depth in this chapter, so let’s take a closer look at each of the key descendants, starting with the encoder branch.
The Encoder Branch 
The first encoder-only model based on the transformer architecture was BERT.
At the time it was published, it broke all state-of-the-art results on the popular GLUE benchmark.
Subsequently, the pretraining objective as well as the architecture of BERT has been adapted to further improve the performance.
Encoder-only models still dominate research and industry on natural language understanding (NLU) tasks such as text classification, named entity recognition, and question-answering.
Let’s have a brief look at the BERT model and its variants: BERT (BERT) is pretrained with the two objectives of predicting masked tokens in texts and determining if two text passages follow each other.
The former task is called masked language modeling (MLM) an the latter next-sentence-prediction (NSP).
BERT used the BookCorpus and English Wikipedia for pretraining and the model can then be fine-tuned on any NLU tasks with very little data.
DistilBERT 
Although BERT delivers great results it can be expensive and difficult to deploy in production due to its sheer size.
By using knowledge distillation during pretraining DistilBERT achieves 97% of BERT’s performance while using 40% less memory and being 60% faster.
You can find more details on knowledge distillation in Chapter 5.
RoBERTa  
A study following the release of BERT revealed that the performance of BERT can be further improved by modifying the pretraining scheme.
RoBERTa is trained longer, on larger batches with more training data and dropped the NSP task to significantly improve the performance over the original BERT model.
XLM 
In the work of XLM, several pretraining objectives for building multilingual models were explored, including the autoregressive language modeling from GPT-like models and MLM from BERT.
In addition, the authors introduced translation language modeling (TLM) which is an extension of MLM to mulitple language inputs.
Experimenting with these pretraining tasks they achieved state of the art on several multilingual NLU benchmarks as well as on translation tasks.
XLM-RoBERTa 
Following the work of XLM and RoBERTa, the XLM-RoBERTA or XLM-R model takes multilingual pretraining one step further by massively up-scaling the training data.
Using the Common Crawl corpus they created a dataset with 2.5 terabytes of text and train an encoder with MLM on this dataset.
Since the dataset only contains monolingual data without any parallel texts, the TLM objective of XLM is dropped.
This approach beats XLM and multilingual BERT variants by a large margin especially on low resource languages.
ALBERT 
The ALBERT model introduced three changes to make the encoder architecture more efficient.
First, it decoupled the token embedding dimension from the hidden dimension, thus allowing the embedding dimension to be small and saving parameters especially when the vocabulary gets large.
Second, all layers share the parameters which decreases the number of effective parameters even further.
Finally, they replace the NSP objective with a sentence-ordering prediction that needs to predict if the order of two sentences was swapped or not rather than prediction if they belong together at all.
These changes allow the training of even larger models that have fewer parameters that show superior performance on NLU tasks.
ELECTRA 
One limitation of the standard MLM pretraining objective is that at each training step only the representations of the masked tokens are updated while the the other input tokens are not.
To address this issue, ELECTRA uses a two model approach: the first model (which is typically small) works like a standard MLM and predicts masked tokens.
The second model called the discriminator is then tasked to predict which of the tokens in the first model output sequence were originally masked.
Therefore, the discriminator needs to make a binary classification for every token which makes training 30 times more efficient.
For downstream tasks the discriminator is fine-tuned like a standard BERT model.
DeBERTa
The DeBERTa model introduces two architectural changes.
On the one hand the authors recognized the importance of position in transformers and disentangled it from the content vector.
With two separate and independent attention mechanisms, both a content and a relative position embedding are processed at each layer.
On the other hand, the absolute position of a word is also important, especially for decoding.
For this reason an absolute position embedding is added just before the SoftMax layer of the token decoding head.
DeBERTa is the first model (as an ensemble) to beat the human baseline on the SuperGLUE benchmark7.
The Decoder Branch
The progress on transformer decoder models has been spearheaded to a large extent by OpenAI.
These models are exceptionally good at predicting  the next word in a sequence and are thus mostly used for text generation tasks (see Chapter 8 for more details).
Their progress has been fueled by using larger datasets and scaling the language models to larger and larger sizes.
Let’s have a look at the evolution of these fascinating generation models: GPT The introduction of GPT combined two key ideas in NLP: the novel and efficient transformer decoder architecture and transfer learning.
In that setup the model is pretrained by predicting the next word based on the context.
The model was trained on the BookCorpus and achieved great results on downstream tasks such as classification.
GPT-2
Inspired by the success of the simple and scalable pretraining approach the original model and training set were up-scaled to produce GPT-2.
This model is able to produce long sequences with coherent text.
Due to concerns of misuse, the model was released in a staged fashion with smaller models being published first and the full model later.
CTRL
Models like GPT-2 can continue given an input sequence or prompt.
However, the user has little control over the style of the generated sequence.
The CTRL model addresses this issue by adding “control tokens” at the beginning of the sequence.
That way the style of the generation can be controlled and allow for diverse generations.
GPT-3
Following the success of scaling GPT up to GPT-2, a thorough survey into the scaling laws of language models8 revealed that there are simple power laws that govern the relation between compute, dataset size, model size and the performance of a language model.
Inspired by these insights, GPT-2 was up-scaled by a factor of 100 to yield GPT-3 with 175 billion parameters.
Besides being able to generate impressively  realistic text passages, the model also exhibits few-shot learning capabilities: with a few examples of a novel task such as text-to-code examples the model is able to accomplish the task on new examples.
OpenAI has not open-sourced this model, but provides an interface through the OpenAI API.
GPT-Neo/GPT-J-6B GPT-Neo and GPT-J-6B are GPT-like models that are trained by EleutherAI, which is a collective of researchers who aim to recreate and release GPT-3 scale models.
The current models are smaller variants of the full 175 billion parameter model, with 2.
7 and 6bn parameters that are competitive with the smaller GPT-3 models OpenAI offers.
The Encoder-Decoder Branch
Although it has become common to build models using a single encoder or decoder stack, there are several encoder-decoder variants of the Transformer that have novel applications across both NLU and NLG domains: T5 The T5 model unifies all NLU and NLG tasks by converting all tasks into text-to-text.
As such all tasks are framed as sequence-to-sequence tasks where adopting an encoder-decoder architecture is natural.
The T5 architecture uses the original Transformer architecture.
Using the large crawled C4 dataset, the model is pre-trained with masked language modeling as well as the SuperGLUE tasks by translating all of them to text-to-text tasks.
The largest model with 11 billion parameters yielded state-of-the-art results on several benchmarks although being comparably large.
BART
BART combines the pretraining procedures of BERT and GPT within the encoder-decoder architecture.
The input sequences undergoes one of  several possible transformation from simple masking, sentence permutation, token deletion to document rotation.
These inputs are passed through the encoder and the decoder has to reconstruct the original texts.
This makes the model more flexible as it is possible to use it for NLU as well as NLG tasks and it achieves state-of-the-artperformance on both.
M2M-100
Conventionally a translation model is built for one language-pair and translation direction.
Naturally, this does not scale to many languages and in addition there might be shared knowledge between language pairs that could be leveraged for translation between rare languages.
M2M-100 is the first translation model that can translate between any of 100 languages.
This allows for high quality translations between rare and underrepresented languages.
BigBird
One main limitation of transformer architectures is the maximum context size due to the quadratic memory requirements of the attention mechanism.
BigBird addresses this issue by using a sparse form of attention that scales linearly.
This allows for the drastic scaling of contexts which is 512 tokens in most BERT models to 4,096 in BigBird.
This is especially useful in cases where long dependencies need to be conserved such as in text summarization.
Conclusion
We started at the heart of the Transformer architecture with a deep-dive into self-attention and subsequently added all the necessary parts to build a transformer encoder model.
We added embedding layers for tokens and positional information, built in a feed forward layer to complement the attention heads and we finally added a classification head to the model body to make predictions.
We also had a look at the decoder side of the  Transformer architecture and concluded the chapter with an overview of the most important model architectures.
With the code that we’ve implemented in this chapter you are well-placed to understand the source code of Transformers and even contribute your first model to the library!
There is a guide in the Transformer documentation that provides you with the information needed to get started.
Now that we have a better understanding of the underlying principles let’s go beyond simple classification and build a question-answering model in the next chapter..
