Chapter 3. Transformer Anatomy
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form—the
author’s raw and unedited content as they write—so you can take
advantage of these technologies long before the official release of these
titles.
This will be the 3rd chapter of the final book. Please note that the
GitHub repo will be made active later on.
If you have comments about how we might improve the content and/or
examples in this book, or if you notice missing material within this
chapter, please reach out to the editor at mpotter@oreilly.com.
Now that we’ve seen what it takes to fine-tune and evaluate a transformer in
Chapter 2, let’s take a look at how they work under the hood. In this chapter
we’ll explore what the main building blocks of transformer models look
like and how to implement them using PyTorch. We first focus on building
the attention mechanism and then add the bits and pieces necessary to make
a transformer encoder work. We also have a brief look at the architectural
differences between the encoder and decoder modules. By the end of this
chapter you will be able to implement a simple transformer model yourself!
While a deep, technical understanding of the transformer architecture is
generally not necessary to use the Transformers library and fine-tune
models to your use-case, it can help understand and navigate the limitations
of the architecture or expand it to new domains.
This chapter also introduces a taxonomy of transformers to help us
understand the veritable zoo of models that has emerged in recent years.

Before diving into the code, let’s start with an overview of the original
architecture that kick-started the transformer revolution.

The Transformer
As we saw in Chapter 1, the original Transformer is based on the encoderdecoder architecture that is widely used for tasks like machine translation,
where a sequence of words is translated from one language to another. This
architecture consists of two components:
Encoder
Converts an input sequence of tokens into a sequence of embedding
vectors, often called the hidden state or context.
Decoder
Uses the encoder’s hidden state to iteratively generate an output
sequence of tokens, one token at a time.
Before the arrival of transformers, the building blocks of the encoder and
decoder were typically recurrent neural networks such as LSTMs,1
augmented with a mechanism called attention.2 Instead of using a fixed
hidden state for the whole input sequence, attention allowed the decoder to
assign a different amount of weight or “attention” to each of the encoder
states at every decoding timestep. By focusing on which input tokens are
most relevant at each timestep, these models were able to learn non-trivial
alignments between the words in a generated translation and those in a
source sentence. For example, Figure 3-1 visualizes the attention weights
for an English to French translation model and shows hows the decoder is
able to correctly align the words “zone” and “Area” which are ordered
differently in the two languages.

Figure 3-1. RNN encoder-decoder alignment of words in the source language (English) and
generated translation (French), where each pixel denotes an attention weight.

Although attention produced much better translations, there was still a
major shortcoming with using recurrent models for the encoder and
decoder: the computations are inherently sequential which prevents
parallelization across tokens in the input sequence.
With the Transformer, a new modeling paradigm was introduced: dispense
with recurrence altogether, and instead rely entirely on a special form of
attention called self-attention. We’ll cover self-attention in more detail later,
but in simple terms it is like attention except that it operates on hidden
states of the same type. So, although the building blocks changed in the
Transformer, the general architecture remained that of an encoder-decoder
as shown in Figure 3-2. This architecture can be trained to convergence
faster than recurrent models and paved the way for many of the recent
breakthroughs in NLP.

Figure 3-2. Encoder-decoder architecture of the Transformer, with the encoder shown in the upper
half of the figure and the decoder in the lower half.

We’ll look at each of the building blocks in detail shortly, but we can
already see a few things in Figure 3-2 that characterize the Transformer
architecture:
The input text is tokenized and converted to token embeddings
using the techniques we encountered in Chapter 2. Since the
attention mechanism is not aware of the relative positions of the
tokens, we need a way to inject some information about token
positions in the input to model the sequential nature of text. The
token embeddings are thus combined with positional embeddings
that contain positional information for each token.
The encoder consists of a stack of encoder layers or “blocks”
which is analogous to stacking convolutional layers in computer
vision. The same is true for the decoder which has its own stack of
decoder layers.
The encoder’s output is fed to each decoder layer, which then
generates a prediction for the most probable next token in the
sequence. The output of this step is then fed back into the decoder
to generate the next token, and so on until a special end-ofsequence token is reached.
The Transformer architecture was originally designed for sequence-tosequence tasks like machine translation, but both the encoder and decoder
submodules were soon adapted as stand-alone models. Although there are
hundreds of different transformer models, most of them belong to one of
three types:
Encoder-only
These models convert an input sequence of text into a rich numerical
representation that is well suited for tasks like text classification or
named entity recognition. BERT and its variants like RoBERTa and
DistilBERT belong to this class of architectures.
Decoder-only

Given a prompt of text like “Thanks for lunch, I had a …”, these models
will auto-complete the sequence by iteratively predicting the most
probable next word. The family of GPT models belong to this class.
Encoder-decoder
Used for modeling complex mappings from one sequence of text to
another. Suitable for machine translation and summarization. The
Transformer, BART and T5 models belong to this class.
NOTE
In reality, the distinction between applications for decoder-only versus encoder-only
architectures is a bit blurry. For example, decoder-only models like those in the GPT
family can be primed for tasks like translation that are conventionally thought of as a
sequence-to-sequence task. Similarly, encoder-only models like BERT can be applied to
summarization tasks that are usually associated with encoder-decoder or decoder-only
models.3

Now that we have a high-level understanding of the Transformer
architecture, let’s take a closer look at the inner workings of the encoder.

Transformer Encoder
As we saw earlier, the Transformer’s encoder consists of many encoder
layers stacked next to each other. As illustrated in Figure 3-3, each encoder
layer receives a sequence of embeddings and feeds them through the
following sub-layers:
A multi-head self-attention layer.
A feed-forward layer.
The output embeddings of each encoder layer have the same size as the
inputs and we’ll soon see that the main role of the encoder stack is to
“update” the input embeddings to produce representations that encode some
contextual information in the sequence.

Figure 3-3. Zooming into the encoder layer.

Each of these sub-layers also has a skip connection and layer normalization,
which are standard tricks to train deep neural networks effectively. But to
truly understand what makes a transformer work we have to go deeper.
Let’s start with the most important building block: the self-attention layer.

Self-Attention
As we discussed earlier in this chapter, self-attention is a mechanism that
allows neural networks to assign a different amount of weight or “attention”
to each element in a sequence. For text sequences, the elements are token
embeddings like the ones we encountered in Chapter 2, where each token is
mapped to a vector of some fixed dimension. For example, in BERT each
token is represented as a 768-dimensional vector. The “self” part of selfattention refers to the fact that these weights are computed for all hidden
states in the same set, e.g. all the hidden states of the encoder. By contrast,
the attention mechanism associated with recurrent models involves
computing the relevance of each encoder hidden state to the decoder hidden
state at a given decoding timestep.
The main idea behind self-attention is that instead of using a fixed
embedding for each token, we can use the whole sequence to compute a
weighted average of each embedding. A simplified way to formulate this is
to say that given a sequence of token embeddings x , ..., x , self-attention
produces a sequence of new embeddings y , ..., y where each y is a linear
combination of all the x :

The coefficients w are called attention weights and are normalized so that
To see why averaging the token embeddings might be a good
idea, consider what comes to mind when you see the word “flies”. You
might think of an annoying insect, but if you were given more context like
“time flies like an arrow” then you would realize that “flies” refers to the
verb instead. Similarly, we can create a representation for “flies” that
incorporates this context by combining all the token embeddings in
different proportions, perhaps by assigning a larger weight w to the token
embeddings for “time” and “arrow”. Embeddings that are generated in this
way are called contextualized embeddings and predate the invention of
transformers with language models like ELMo4. A cartoon of the process is
shown in Figure 3-4 where we illustrate how, depending on the context, two
different representations for “flies” can be generated via self-attention.

Figure 3-4. Cartoon of how self-attention updates raw token embeddings (upper) into contextualized
embeddings (lower) to create representations that incorporate information from the whole sequence.

Let’s now take a look at how we can calculate the attention weights.
Scaled Dot-Product Attention
There are several ways to implement a self-attention layer, but the most
common one is scaled dot-product attention from the Attention is All You
Need paper where the Transformer was introduced. There are four main
steps needed to implement this mechanism:
Create query, key, and value vectors
Each token embedding is projected into three vectors called query, key,
and value.
Compute attention scores
Determine how much the query and key vectors relate to each other
using a similarity function. As the name suggests, the similarity function
for scaled dot-product attention is a dot-product matrix multiplication of
the embeddings. Queries and keys that are similar will have a large dotproduct, while those that don’t share much in common will have little to
no overlap. The outputs from this step are called the attention scores
and for a sequence with n input tokens, there is a corresponding n × n
matrix of attention scores.

Compute attention weights
Dot-products can in general produce arbitrarily large numbers which
can destabilize the training process. To handle this, the attention scores
are first multiplied by a scaling factor and then normalized with a
softmax to ensure all the column values sum to one. The resulting n × n
matrix now contains all the attention weights w .
ji

Update the token embeddings
Once the attention weights are computed, we multiply them by the
value vector to obtain an updated representation for embedding
We can visualize how the attention weights are calculated with a nifty
library called BertViz. This library provides several functions that can be
used for visualizing different aspects of attention in Transformers models.
To visualize the attention weights, we can use the neuron_view module
which traces the computation of the weights to show how the query and key
vectors are combined to produce the final weight. Since BertViz needs to
tap into the attention layers of the model, we’ll instantiate our BERT
checkpoint with their model class and then use the show function to
generate the interactive visualization:

DEMYSTIFYING QUERIES, KEYS, AND VALUES
The notion of query, key, and value vectors can be a bit cryptic the first
time you encounter them - for instance, why are they called that? The
origin of these names is inspired from information retrieval systems, but
we can motivate their meaning with a simple analogy: imagine that
you’re at the supermarket buying all the ingredients necessary for your
dinner. From the dish’s recipe, each of the required ingredients can be
thought of as a query, and as you scan through the shelves you look at
the labels (keys) and check if it matches an ingredient on your list
(similarity function). If you have a match then you take the item (value)
from the shelf.
In this example, we only get one grocery item for every label that
matches the ingredient. Self-attention is a more abstract and “smooth”
version of this: every label in the supermarket matches the ingredient to
the extent to which each key matches the query.
Let’s take a look at this process in more detail by implementing the diagram
of operations to compute scaled dot-product attention as shown in Figure 35.

Figure 3-5. Operations in scaled dot-product attention.

The first thing we need to do is tokenize the text, so let’s use our tokenizer
to extract the input IDs:

As we saw in Chapter 2, each token in the sentence has been mapped to a
unique ID in the tokenizer’s vocabulary. To keep things simple, we’ve also
excluded the [CLS] and [SEP] tokens. Next we need to create some dense
embeddings. In PyTorch, we can do this by using a
torch.nn.Embedding layer that acts as a lookup table for each input
ID:

Here we’ve used the AutoConfig class to load the config.json file
associated with the bert-base-uncased checkpoint. In Transformers,
every checkpoint is assigned a configuration file that specifies various

hyperparameters like vocab_size and hidden_size, which in our
example shows us that each input ID will be mapped to one of the 30,522
embedding vectors stored in nn.Embedding, each with a size of 768.
Now we have our lookup table we can generate the embeddings by feeding
the input IDs:

This has given us a tensor of size (batch_size, seq_len,
hidden_dim), just like we saw in Chapter 2. We’ll postpose the
positional encodings for later, so the next step is to create the query, key,
and value vectors and calculate the attention scores using the dot-product as
the similarity function:

We’ll see later that the query, key, and value vectors are generated by
applying independent weight matrices W
to the embeddings, but for
now we’ve kept them equal for simplicity. In scaled dot-product attention,
the dot-products are scaled by the size of the embedding vectors so that we
don’t get too many large numbers during training that can cause problems
with back propagation:

NOTE
The torch.bmm function performs a batch matrix-matrix product that simplifies the
computation of the attention scores where the query and key vectors have size
(batch_size, seq_len, hidden_dim). If we ignored the batch dimension we
could calculate the dot product between each query and key vector by simply
transposing the key tensor to have shape (hidden_dim, seq_len) and then using
the matrix product to collect all the dot-products in a (seq_len, seq_len) matrix.
Since we want to do this for all sequences in the batch independently, we use
torch.bmm which simply prepends the batch dimension to the matrix dimensions.

This has created a 5 × 5 matrix of attention scores. Next we normalize them
by applying a softmax so the sum over each column is equal to one.

The final step is to multiply the attention weights by the values.

And that’s it - we’ve gone through all the steps to implement a simplified
form of self-attention! Notice that the whole process is just two matrix
multiplications and a softmax, so next time you think of “self-attention”
you can mentally remember that all we’re doing is just a fancy form of
averaging.
Let’s wrap these steps in a function that we can use later.

Our attention mechanism with equal query and key vectors will assign a
very large score to identical words in the context, and in particular to the
current word itself: the dot product of a query with itself is always 1. But in
practice the meaning of a word will be better informed by complementary
words in the context than by identical words, e.g. the meaning of “flies” is
better defined by incorporating information from “time” and “arrow” than
by another mention of “flies”. How can we promote this behavior?
Let’s allow the model to create a different set of vectors for the query, key
and value of a token by using three different linear projections to project
our initial token vector into three different spaces.
Multi-Headed Attention
In our simple example, we only used the embeddings “as is” to compute the
attention scores and weights, but that’s far from the whole story. In practice,
the self-attention layer applies three independent linear transformations to
each embedding to generate the query, key, and value vectors. These
transformations project the embeddings and each projection carries its own
set of learnable parameters, which allows the self-attention layer to focus on
different semantic aspects of the sequence.
It also turns out to be beneficial to have multiple sets of linear projections,
each one representing a so-called attention head. The resulting multiheaded attention layer is illustrated in Figure 3-6.

Figure 3-6. Multi-headed attention.

Let’s implement this layer by first coding up a single attention head.

Here we’ve initialized three independent linear layers that apply matrix
multiplication to the embedding vectors to produce tensors of size
(batch_size, seq_len, head_dim) where head_dim is the
dimension we are projecting into. Although head_dim does not have to be
smaller than the embedding dimension embed_dim of the tokens, in
practice it is chosen to be a multiple of embed_dim so that the
computation across each head is constant. For example in BERT has 12
attention heads, so the dimension of each head is 768/12 = 64.
Now that we have a single attention head, we can concatenate the outputs of
each one to implement the full multi-headed attention layer.

Notice that the concatenated output from the attention heads is also fed
through a final linear layer to produce an output tensor of size
(batch_size, seq_len, hidden_dim) that is suitable for the
feed forward network downstream. As a sanity check, let’s see if the multiheaded attention produces the expected shape of our inputs.

It works! To wrap up this section on attention, let’s use BertViz again to
visualise the attention for two different uses of the word “flies”. Here we
can use the head_view function from BertViz by computing the
attentions, tokens and indicating where the sentence boundary lies.

This visualization shows the attention weights as lines connecting the token
whose embedding is getting updated (left), with every word that is being
attended to (right). The intensity of the lines indicates the strength of the
attention weights, with values close to 1 dark, and faint lines close to zero.

In this example, the input consists of two sentences and the [CLS] and
[SEP] tokens are the special tokens in BERT’s tokenizer that we
encountered in Chapter 2. One thing we can see from the visualization is
the attention weights are strongest between words that belong to the same
sentence, which suggests BERT can tell that it should attend to words in the
same sentence. However, for the word “flies” we can see that BERT has
identified “arrow” and important in the first sentence and “fruit” and
“banana” in the second. These attention weights allow the model to
distinguish the use of “flies” as a verb or noun, depending on the context it
occurs!
Now that we’ve covered attention, let’s take a look at implementing the
missing piece of the encoder layer: position-wise feed forward networks.

Feed Forward Layer
The feed forward sub-layer in the encoder and decoder is just a simple 2layer fully-connected neural network, but with a twist; instead of processing
the whole sequence of embeddings as a single vector, it processes each
embedding independently. For this reason, this layer is often referred to as a
position-wise feed forward layer. These position-wise feed forward layers
are sometimes also referred to as a one-dimensional convolution with
kernel size of one, typically by people with a computer vision background
(e.g. the OpenAI GPT codebase uses this nomenclature). A rule of thumb
from the literature is to pick the hidden size of the first layer to be four
times the size of the embeddings and a GELU activation function is most
commonly used. This is where most of the capacity and memorization is
hypothesized to happen and the part that is most often scaled when scaling
up the models.

We now have all the ingredients to create a fully-fledged transformer
encoder layer! The only decision left to make is where to place the skip
connections and layer normalization. Let’s take a look and how this affect
the model architecture.

Putting It All Together
When it comes to placing the layer normalization in the encoder or decoder
layers of a transformer, there are two main choices adopted in the literature:
Post layer normalization
This is the arrangement from the Transformer paper and places layer
normalization in between the skip connections. This arrangement is
tricky to train from scratch as the gradients can diverge. For this reason,
you will often see a concept known as learning rate warm-up, where the
learning rate is gradually increased from a small value to some
maximum during training.
Pre layer normalization

The most common arrangement found in the literature and places layer
normalization in between the skip connections. Tends to be much more
stable during training and does not usually require learning rate
warmup.
The difference between the two arrangements is illustrated in Figure 3-7.

Figure 3-7. Different arrangements of layer normalization in a transformer encoder layer.

Let’s now test this with our input embeddings.

It works! We’ve now implemented our very first transformer encoder layer
from scratch! In principle we could now pass the input embeddings through
the encoder layer. However, there is a caveat with the way we setup the
encoder layers: they are totally invariant to the position of the tokens. Since
the multi-head attention layer is effectively a fancy weighted sum, there is
no way to encode the positional information in the sequence.5
Luckily there is an easy trick to incorporate positional information with
positional encodings. Let’s take a look.

Positional Embeddings

Positional embeddings are based on a simple, yet very effective idea:
augment the token embeddings with a position-dependent pattern of values
arranged in a vector. If the pattern is characteristic for each position, the
attention heads and feed-forward layers in each stack can learn to
incorporate positional information in their transformations.
There are several ways to achieve this and one of the most popular
approaches, especially when the pretraining dataset is sufficiently large, is
to use a learnable pattern. This works exactly the same way as the token
embeddings but using the position index instead of the token ID as input.
With that approach an efficient way of encoding the position of tokens is
learned during pretraining.
Let’s create a custom Embeddings module that combines a token
embedding layer that projects the input_ids to a dense hidden state
together with the positional embedding that does the same for
position_ids. The resulting embedding is simply the sum of both
embeddings.

We see that the embedding layer now creates a single, dense embedding for
each token. While learnable position embeddings are easy to implement and
widely used there are several alternatives:
Absolute positional representations
The Transformer model uses static patterns to encode the position of the
tokens. The pattern consists of modulated sine and cosine signals and
works especially well in the low data regime.
Relative positional representations
Although absolute positions are important one can argue that for
computing a token embedding mostly the relative position to the token
is important. Relative positional representations follow that intuition
and encode the relative positions between tokens. Models such as
DeBERTa use such representations.
Rotary position embeddings
By combining the idea of absolute and relative positional
representations rotary position embeddings achieve excellent results on
many tasks. A recent example of rotary position embeddings in action is
GPT-Neo.
Let’s put it all together now by building the full transformer encoder by
combining the embeddings with the encoder layers.

We can see that we get a hidden state for each token in the batch. This
output format makes the architecture very flexible and we can easily adapt
it for various applications such as predicting missing tokens in masked
langauge modeling or predicting start and end position of an answer in
question-answering. Let’s see how we can build a classifier with the
encoder like the one we used in Chapter 2 in the following section.

Bodies and Heads
So now that we have a full transformer encoder model we would like to
build a classifier with it. The model is usually divided into a task
independant body and a task specific head. What we’ve built so far is the
body and we now need to attach a classification head to that body. Since we
have a hidden state for each token but only need to make one prediction
there are several option how to approach this. Traditionally, the first token
in such models is used for the prediction and we can attach a dropout and
linear layer to make a classification prediction. The following class extends
the existing encoder for sequence classification.

Before initializing the model we need to define how many classes we would
like to predict.

That is exactly what we have been looking for. For each example in the
batch we get the un-normalized logits for each class in the output. This
corresponds to the BERT model that we used in Chapter 2 to detect
emotions in tweets.
This concludes our analysis of the encoder, so let’s now cast our attention
(pun intended!) to the decoder.

Transformer Decoder
As illustrated in Figure 3-8, the main difference between the decoder and
encoder is that the decoder has two attention sublayers:
Masked multi-head attention

Ensures that the tokens we generate at each timestep are only based on
the past outputs and the current token being predicted. Without this, the
decoder could cheat during training by simply copying the target
translations, so masking the inputs ensures the task is not trivial.
Encoder-decoder attention
Performs multi-head attention over the output key and value vectors of
the encoder stack, with the intermediate representation of the decoder
acting as the queries. This way the encoder-decoder attention layer
learns how to relate tokens from two different sequences such as two
different languages.

Figure 3-8. Zooming into the Transformer decoder layer.

Let’s take a look at the modifications we need to include masking in selfattention, and leave the implementation of the encoder-decoder attention
layer as a homework problem. The trick with masked self-attention is to
introduce a mask matrix with ones on the lower diagonal and zeros above.

Here we’ve used PyTorch’s tril function to create the lower triangular
matrix. Once we have this mask matrix, we can the prevent each attention
head from peeking at future tokens by using
torch.Tensor.masked_fill to replace all the zeros with negative
infinity.

By setting the upper values to negative infinity, we guarantee that the
attention weights are all zero once we take the softmax over the scores
because e = 0. We can easily include this masking behavior with a small
change to our scaled dot-product attention function that we implemented
earlier in this chapter.

From here it is a simple matter to build up the decoder layer and we point
the reader to the excellent implementation of minGPT by Andrej Karpathy
for details. Okay this was quite a lot of technical detail, but now we have a
good understanding on how every piece of the Transformer architecture
works. Let’s round out the chapter by stepping back a bit and looking at the
landscape of different transformer models and how they relate to each other.

Meet the Transformers

As we have seen in this chapter there are three main architectures for
transformer models: encoders, decoders, and encoder-decoders. The initial
success of the early transformer models triggered a Cambrian explosion in
model development as researchers built models on various datasets of
different size and nature, used new pretraining objectives, and tweaked the
architecture to further improve performance. Although the zoo of models is
still growing fast, the wide variety of models can still be divided into the
three categories of encoders, decoders, and encoder-decoders.
In this section we’ll provide a brief overview of the most important
transformer models. Let’s start by taking a look at the transformer family
tree.

The Transformer Tree of Life
Over time, each of three main architecture have undergone an evolution of
their own which is illustrated in Figure 3-9 where a few of the most
prominent models and their descendants is shown.

Figure 3-9. An overview of some of the most prominent transformer architectures.

With over 50 different architectures included in Transformers, this family
tree by no means provides a complete overview of all the existing
architectures, and simply highlights a few of the architectural milestones.
We’ve covered the Transformer in depth in this chapter, so let’s take a
closer look at each of the key descendants, starting with the encoder branch.

The Encoder Branch
The first encoder-only model based on the transformer architecture was
BERT. At the time it was published, it broke all state-of-the-art results on
the popular GLUE benchmark.6 Subsequently, the pretraining objective as
well as the architecture of BERT has been adapted to further improve the
performance. Encoder-only models still dominate research and industry on
natural language understanding (NLU) tasks such as text classification,
named entity recognition, and question-answering. Let’s have a brief look at
the BERT model and its variants:
BERT
(BERT) is pretrained with the two objectives of predicting masked
tokens in texts and determining if two text passages follow each other.
The former task is called masked language modeling (MLM) an the
latter next-sentence-prediction (NSP). BERT used the BookCorpus and
English Wikipedia for pretraining and the model can then be fine-tuned
on any NLU tasks with very little data.
DistilBERT
Although BERT delivers great results it can be expensive and difficult
to deploy in production due to its sheer size. By using knowledge
distillation during pretraining DistilBERT achieves 97% of BERT’s
performance while using 40% less memory and being 60% faster. You
can find more details on knowledge distillation in Chapter 5.
RoBERTa

A study following the release of BERT revealed that the performance of
BERT can be further improved by modifying the pretraining scheme.
RoBERTa is trained longer, on larger batches with more training data
and dropped the NSP task to significantly improve the performance
over the original BERT model.
XLM
In the work of XLM, several pretraining objectives for building
multilingual models were explored, including the autoregressive
language modeling from GPT-like models and MLM from BERT. In
addition, the authors introduced translation language modeling (TLM)
which is an extension of MLM to mulitple language inputs.
Experimenting with these pretraining tasks they achieved state of the art
on several multilingual NLU benchmarks as well as on translation tasks.
XLM-RoBERTa
Following the work of XLM and RoBERTa, the XLM-RoBERTA or
XLM-R model takes multilingual pretraining one step further by
massively up-scaling the training data. Using the Common Crawl
corpus they created a dataset with 2.5 terabytes of text and train an
encoder with MLM on this dataset. Since the dataset only contains
monolingual data without any parallel texts, the TLM objective of XLM
is dropped. This approach beats XLM and multilingual BERT variants
by a large margin especially on low resource languages.
ALBERT
The ALBERT model introduced three changes to make the encoder
architecture more efficient. First, it decoupled the token embedding
dimension from the hidden dimension, thus allowing the embedding
dimension to be small and saving parameters especially when the
vocabulary gets large. Second, all layers share the parameters which
decreases the number of effective parameters even further. Finally, they
replace the NSP objective with a sentence-ordering prediction that
needs to predict if the order of two sentences was swapped or not rather
than prediction if they belong together at all. These changes allow the
training of even larger models that have fewer parameters that show
superior performance on NLU tasks.
ELECTRA
One limitation of the standard MLM pretraining objective is that at each
training step only the representations of the masked tokens are updated
while the the other input tokens are not. To address this issue,
ELECTRA uses a two model approach: the first model (which is
typically small) works like a standard MLM and predicts masked
tokens. The second model called the discriminator is then tasked to
predict which of the tokens in the first model output sequence were
originally masked. Therefore, the discriminator needs to make a binary
classification for every token which makes training 30 times more
efficient. For downstream tasks the discriminator is fine-tuned like a
standard BERT model.
DeBERTa
The DeBERTa model introduces two architectural changes. On the one
hand the authors recognized the importance of position in transformers
and disentangled it from the content vector. With two separate and
independent attention mechanisms, both a content and a relative
position embedding are processed at each layer. On the other hand, the
absolute position of a word is also important, especially for decoding.
For this reason an absolute position embedding is added just before the
SoftMax layer of the token decoding head. DeBERTa is the first model
(as an ensemble) to beat the human baseline on the SuperGLUE
benchmark7.

The Decoder Branch
The progress on transformer decoder models has been spearheaded to a
large extent by OpenAI. These models are exceptionally good at predicting

the next word in a sequence and are thus mostly used for text generation
tasks (see Chapter 8 for more details). Their progress has been fueled by
using larger datasets and scaling the language models to larger and larger
sizes. Let’s have a look at the evolution of these fascinating generation
models:
GPT
The introduction of GPT combined two key ideas in NLP: the novel and
efficient transformer decoder architecture and transfer learning. In that
setup the model is pretrained by predicting the next word based on the
context. The model was trained on the BookCorpus and achieved great
results on downstream tasks such as classification.
GPT-2
Inspired by the success of the simple and scalable pretraining approach
the original model and training set were up-scaled to produce GPT-2.
This model is able to produce long sequences with coherent text. Due to
concerns of misuse, the model was released in a staged fashion with
smaller models being published first and the full model later.
CTRL
Models like GPT-2 can continue given an input sequence or prompt.
However, the user has little control over the style of the generated
sequence. The CTRL model addresses this issue by adding “control
tokens” at the beginning of the sequence. That way the style of the
generation can be controlled and allow for diverse generations.
GPT-3
Following the success of scaling GPT up to GPT-2, a thorough survey
into the scaling laws of language models8 revealed that there are simple
power laws that govern the relation between compute, dataset size,
model size and the performance of a language model. Inspired by these
insights, GPT-2 was up-scaled by a factor of 100 to yield GPT-3 with
175 billion parameters. Besides being able to generate impressively

realistic text passages, the model also exhibits few-shot learning
capabilities: with a few examples of a novel task such as text-to-code
examples the model is able to accomplish the task on new examples.
OpenAI has not open-sourced this model, but provides an interface
through the OpenAI API.
GPT-Neo/GPT-J-6B
GPT-Neo and GPT-J-6B are GPT-like models that are trained by
EleutherAI, which is a collective of researchers who aim to recreate and
release GPT-3 scale models. The current models are smaller variants of
the full 175 billion parameter model, with 2.7 and 6bn parameters that
are competitive with the smaller GPT-3 models OpenAI offers.

The Encoder-Decoder Branch
Although it has become common to build models using a single encoder or
decoder stack, there are several encoder-decoder variants of the
Transformer that have novel applications across both NLU and NLG
domains:
T5
The T5 model unifies all NLU and NLG tasks by converting all tasks
into text-to-text. As such all tasks are framed as sequence-to-sequence
tasks where adopting an encoder-decoder architecture is natural. The T5
architecture uses the original Transformer architecture. Using the large
crawled C4 dataset, the model is pre-trained with masked language
modeling as well as the SuperGLUE tasks by translating all of them to
text-to-text tasks. The largest model with 11 billion parameters yielded
state-of-the-art results on several benchmarks although being
comparably large.
BART
BART combines the pretraining procedures of BERT and GPT within
the encoder-decoder architecture. The input sequences undergoes one of

several possible transformation from simple masking, sentence
permutation, token deletion to document rotation. These inputs are
passed through the encoder and the decoder has to reconstruct the
original texts. This makes the model more flexible as it is possible to
use it for NLU as well as NLG tasks and it achieves state-of-the-artperformance on both.
M2M-100
Conventionally a translation model is built for one language-pair and
translation direction. Naturally, this does not scale to many languages
and in addition there might be shared knowledge between language
pairs that could be leveraged for translation between rare languages.
M2M-100 is the first translation model that can translate between any of
100 languages. This allows for high quality translations between rare
and underrepresented languages.
BigBird
One main limitation of transformer architectures is the maximum
context size due to the quadratic memory requirements of the attention
mechanism. BigBird addresses this issue by using a sparse form of
attention that scales linearly. This allows for the drastic scaling of
contexts which is 512 tokens in most BERT models to 4,096 in BigBird.
This is especially useful in cases where long dependencies need to be
conserved such as in text summarization.

Conclusion
We started at the heart of the Transformer architecture with a deep-dive into
self-attention and subsequently added all the necessary parts to build a
transformer encoder model. We added embedding layers for tokens and
positional information, built in a feed forward layer to complement the
attention heads and we finally added a classification head to the model body
to make predictions. We also had a look at the decoder side of the

Transformer architecture and concluded the chapter with an overview of the
most important model architectures.
With the code that we’ve implemented in this chapter you are well-placed
to understand the source code of Transformers and even contribute your
first model to the library! There is a guide in the Transformer
documentation that provides you with the information needed to get started.
Now that we have a better understanding of the underlying principles let’s
go beyond simple classification and build a question-answering model in
the next chapter.
